{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Alison Araujo\n",
    "\n",
    "Nome: Gabrielly Carneiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atenção: Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Carregando algumas bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gabri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading word_tokenize: Package 'word_tokenize' not\n",
      "[nltk_data]     found in index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\gabri\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (2.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting pt-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.6.0/pt_core_news_sm-3.6.0-py3-none-any.whl (13.0 MB)\n",
      "     ---------------------------------------- 13.0/13.0 MB 7.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from pt-core-news-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (63.4.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (21.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (4.64.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.3.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.21.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gabri\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('word_tokenize')\n",
    "stopwordsdic = stopwords.words('portuguese')\n",
    "\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "\n",
    "from spacy import load\n",
    "nlp = load('pt_core_news_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperamos trabalhar no diretório\n",
      "c:\\Users\\gabri\\OneDrive - Insper - Institudo de Ensino e Pesquisa\\Projeto 1\\C.DadosP1\n"
     ]
    }
   ],
   "source": [
    "print('Esperamos trabalhar no diretório')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando a base de dados com os tweets classificados manualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mensagem</th>\n",
       "      <th>Acionável (para amazon)/direcionável (para editora)/não acionável(para o autor ou irrelevante)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Para quem gosta de poemas simples, esse é o li...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quando vi o lançamento pensei que finalmente p...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>É incrível como esses escritores e \"intelectua...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Se desse pra devolver eu devolvia, nao é por e...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Futilidade, inutilidade, desperdício de papel,...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Mensagem  \\\n",
       "0  Para quem gosta de poemas simples, esse é o li...   \n",
       "1  Quando vi o lançamento pensei que finalmente p...   \n",
       "2  É incrível como esses escritores e \"intelectua...   \n",
       "3  Se desse pra devolver eu devolvia, nao é por e...   \n",
       "4  Futilidade, inutilidade, desperdício de papel,...   \n",
       "\n",
       "  Acionável (para amazon)/direcionável (para editora)/não acionável(para o autor ou irrelevante)  \n",
       "0                                                  N                                              \n",
       "1                                                  D                                              \n",
       "2                                                  N                                              \n",
       "3                                                  N                                              \n",
       "4                                                  N                                              "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_excel('dados_treino.xlsx')\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mensagem</th>\n",
       "      <th>Acionável (para amazon)/direcionável (para editora)/não acionável(autor ou irrelevante)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O livro é prolixo, redundante, doentio. Sou su...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bom livro e história envolvente. Porém, o leit...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fala tudo que todos já sabem, sem falar que nã...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ganhei na compra do Kindle. Não é estilo de li...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Custa crer que um livro tão medíocre, embora m...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Mensagem  \\\n",
       "0  O livro é prolixo, redundante, doentio. Sou su...   \n",
       "1  Bom livro e história envolvente. Porém, o leit...   \n",
       "2  Fala tudo que todos já sabem, sem falar que nã...   \n",
       "3  Ganhei na compra do Kindle. Não é estilo de li...   \n",
       "4  Custa crer que um livro tão medíocre, embora m...   \n",
       "\n",
       "  Acionável (para amazon)/direcionável (para editora)/não acionável(autor ou irrelevante)  \n",
       "0                                                  N                                       \n",
       "1                                                  D                                       \n",
       "2                                                  N                                       \n",
       "3                                                  N                                       \n",
       "4                                                  N                                       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_excel('dados_teste.xlsx')\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Classificador automático\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na primeira etapa, de classificação manual, consideramos três targets para os reviews: Acionável, Direcionável e Não Acionável. \n",
    "\n",
    "- Acionável: para ser considerado \"acionável\" (\"A\") o review deve ser passível de alguma ação pela Amazon, ou seja, o review deve ser sobre entrega, estado do produto, contato com o suporte, etc. \n",
    "- Direcionável: para o target \"direcionável\" (\"D\") foram considerados comentários relativos à editora, como qualidade do material do livro, preço do livro e do e-book, tradução e edição. \n",
    "- Não Acionáveis: por fim, os não acionáveis (\"NA\") eram comentários relativos ao autor, ao apreço pelo conteúdo do livro, ou comentários irrelavantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando um Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar uma função que transforma as frases da planilha em um texto só \n",
    "    #(Será útil para criar o dicionário com as palavras)\n",
    "def transforma_em_string(coluna):\n",
    "    texto = ''\n",
    "    for linha in coluna:\n",
    "        texto += linha + ' '\n",
    "    return texto    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar uma função que limpa todas as pontuações\n",
    "def cleanup(text):\n",
    "    punctuation = r'[´\"!-.:?;$,/~^_=+*&¨%$#@|\\{}()[\\]]'\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, '', text)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria uma função que limpa os espaços duplicados\n",
    "def limpa_espaco(text):\n",
    "    punctuation = r'[\\n]'  # Adicione os caracteres desejados aqui\n",
    "    pattern = re.compile(punctuation)\n",
    "    text_subbed = re.sub(pattern, '', text)\n",
    "    return text_subbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando uma função para remover emoji\n",
    "def remove_emoji(text):\n",
    "    text_without_emojis = unidecode(text)\n",
    "    return text_without_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a função de stopwords\n",
    "def stopwords(texto):\n",
    "    palavras = word_tokenize(texto, language='portuguese') # Tokenize é analisar palavras individualmente, basicamente\n",
    "    palavras_sem_stopword = []\n",
    "    for palavra in palavras:\n",
    "        if palavra not in stopwordsdic:\n",
    "            palavras_sem_stopword.append(palavra)\n",
    "    # Reúna as palavras sem stopwords em uma string novamente\n",
    "    texto_sem_stopword = ' '.join(palavras_sem_stopword)\n",
    "    return texto_sem_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cria uma função que reúna as funções de limpeza\n",
    "def limpa_tudo(mensagem):\n",
    "    #Aplicando a função de limpeza de pontuação\n",
    "    texto = cleanup(mensagem)\n",
    "    #Deixando tudo em letra minúscula\n",
    "    texto = texto.lower()\n",
    "    #Removendo emoji\n",
    "    texto = remove_emoji(texto)\n",
    "    #Aplicando a função de limpeza de espaço\n",
    "    texto = limpa_espaco(texto)\n",
    "    #Removendo stopwords\n",
    "    texto = stopwords(texto)\n",
    "    #Realiza lemmatização\n",
    "    \n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria uma função que adicione as mensagens limpas à planilha\n",
    "def mensagem_limpa(planilha):   #recebe a planilha e retorna com a coluna de mensagem limpa\n",
    "    planilha['Mensagem Limpa'] = [limpa_tudo(x) for x in list(planilha['Mensagem'])]\n",
    "    return planilha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #separando cada palavra\n",
    "    #texto = texto.split()\n",
    "    #set para tirar repetição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria dicionário de tudo\n",
    "def cria_vocabulario(mensagem):\n",
    "    lista_palavras = transforma_em_string(mensagem)\n",
    "    lista_palavras = limpa_tudo(lista_palavras)\n",
    "    lista_palavras = lista_palavras.split()\n",
    "    return lista_palavras  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma função que guarda as palavras em pd.Series\n",
    "def cria_pdseries(lista):\n",
    "    tabela = pd.Series(lista)\n",
    "    return tabela\n",
    "\n",
    "#Cria uma função que retorna a frequência absoluta de cada palavra no texto\n",
    "def freq_abs(tabela):\n",
    "    absoluta = tabela.value_counts()\n",
    "    return absoluta\n",
    "\n",
    "#Cria uma função que retorna a frequência relativa de cada palavra no texto\n",
    "def freq_rel(tabela):\n",
    "    relativa = tabela.value_counts(True)\n",
    "    return relativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mesmas',\n",
       " 'popular',\n",
       " 'esperei',\n",
       " 'voltas',\n",
       " 'chamala',\n",
       " 'devolvia',\n",
       " 'ano',\n",
       " 'alfredo',\n",
       " 'alcanca',\n",
       " 'registroa',\n",
       " 'eternidade',\n",
       " 'costumam',\n",
       " 'elegancia',\n",
       " 'atraves',\n",
       " 'faz',\n",
       " 'anotacoes',\n",
       " 'riquissimas',\n",
       " 'jeito',\n",
       " 'digo',\n",
       " 'adequadamente',\n",
       " 'transforma',\n",
       " 'indecisoes',\n",
       " 'acompanha',\n",
       " 'aliasduvido',\n",
       " 'energia',\n",
       " 'aprofundar',\n",
       " 'anti',\n",
       " 'sacadas',\n",
       " 'pnl',\n",
       " 'apaixonaessa',\n",
       " 'unlimited',\n",
       " 'unavailable',\n",
       " 'solucao',\n",
       " 'instrucao',\n",
       " 'absurdas',\n",
       " 'costuman',\n",
       " 'trilogia',\n",
       " 'pensadores',\n",
       " 'quis',\n",
       " 'china',\n",
       " 'novos',\n",
       " 'bastava',\n",
       " 'enrolada',\n",
       " '8',\n",
       " 'baixissima',\n",
       " 'reafirmados',\n",
       " 'pai',\n",
       " 'fe',\n",
       " 'contraponto',\n",
       " 'produtivas',\n",
       " 'ler',\n",
       " 'etc',\n",
       " 'achava',\n",
       " 'definitivo',\n",
       " 'impiricas',\n",
       " '8020',\n",
       " 'meninada',\n",
       " 'autoresponsabilidade',\n",
       " 'conseguiu',\n",
       " 'toda',\n",
       " 'alegase',\n",
       " 'tecnico',\n",
       " 'cru',\n",
       " 'tetraplegico',\n",
       " 'quietos',\n",
       " 'vao',\n",
       " 'famosas',\n",
       " 'escolares',\n",
       " 'demasiadamente',\n",
       " 'dinheiro',\n",
       " 'focado',\n",
       " 'dramatica',\n",
       " 'eg',\n",
       " 'falsifica',\n",
       " 'historias',\n",
       " 'imatura',\n",
       " 'desequilibrada',\n",
       " 'tratados',\n",
       " 'chorar',\n",
       " 'virou',\n",
       " 'frustrada',\n",
       " 'propagar',\n",
       " 'funcionar',\n",
       " 'pensar',\n",
       " 'populacoes',\n",
       " 'valido',\n",
       " 'ribeiro',\n",
       " 'sofrimento',\n",
       " 'comparacoes',\n",
       " 'maria',\n",
       " 'umberto',\n",
       " 'chegouquero',\n",
       " 'bill',\n",
       " 'lombada',\n",
       " 'custos',\n",
       " 'proposital',\n",
       " 'conservador',\n",
       " 'trem',\n",
       " 'inumero',\n",
       " 'cursou',\n",
       " 'dificuldades',\n",
       " 'antecipadamente',\n",
       " 'beneficiam',\n",
       " 'entidades',\n",
       " 'caixa',\n",
       " 'viu',\n",
       " 'bukowsk',\n",
       " 'espacamento',\n",
       " 'way',\n",
       " 'especificos',\n",
       " 'cronologico',\n",
       " 'totalitarias',\n",
       " 'harper',\n",
       " 'tantas',\n",
       " 'filosofica',\n",
       " 'ficaria',\n",
       " 'toca',\n",
       " 'premissa',\n",
       " 'recalque',\n",
       " 'categoria',\n",
       " 'apresentase',\n",
       " 'vexame',\n",
       " 'novo',\n",
       " 'viveria',\n",
       " 'procurei',\n",
       " 'relatos',\n",
       " 'socialista',\n",
       " 'tao',\n",
       " 'universidades',\n",
       " 'rasgo',\n",
       " 'ta',\n",
       " 'enquanto',\n",
       " 'antigo',\n",
       " 'sac',\n",
       " 'scott',\n",
       " 'compadrios',\n",
       " 'obter',\n",
       " 'oi',\n",
       " 'suficiente',\n",
       " 'elaborada',\n",
       " 'historia',\n",
       " 'porem',\n",
       " 'solicitacao',\n",
       " 'cria',\n",
       " 'software',\n",
       " 'devida',\n",
       " 'imaginava',\n",
       " 'iliada',\n",
       " 'encadernacao',\n",
       " 'observou',\n",
       " 'delirios',\n",
       " 'enxugando',\n",
       " 'voltados',\n",
       " 'saraivapois',\n",
       " 'prevista',\n",
       " 'respectivos',\n",
       " 'impor',\n",
       " 'confirmar',\n",
       " 'alemao',\n",
       " 'recortes',\n",
       " 'precisando',\n",
       " 'latinoamericanos',\n",
       " 'aumentar',\n",
       " 'arcaicas',\n",
       " 'substantivos',\n",
       " 'conferir',\n",
       " 'autorizo',\n",
       " 'dormir',\n",
       " 'factiveis',\n",
       " 'informar',\n",
       " 'crimes',\n",
       " 'entendimento',\n",
       " 'cassino',\n",
       " '576',\n",
       " 'imbecil',\n",
       " 'reconhecerem',\n",
       " 'saber',\n",
       " 'historica',\n",
       " 'sensiveis',\n",
       " 'web',\n",
       " 'angulos',\n",
       " 'levou',\n",
       " 'vez',\n",
       " 'estupro',\n",
       " 'rubens',\n",
       " 'chatas',\n",
       " 'livrooooo',\n",
       " 'grudada',\n",
       " 'contrario',\n",
       " 'digital',\n",
       " 'stalin',\n",
       " 'conspiracao',\n",
       " 'muda',\n",
       " 'mora',\n",
       " 'trepadas',\n",
       " 'arrumarem',\n",
       " 'burca',\n",
       " 'cosmos',\n",
       " 'criado',\n",
       " 'familias',\n",
       " '08012018',\n",
       " 'levitsky',\n",
       " 'surreal',\n",
       " 'atestar',\n",
       " 'envolvidos',\n",
       " 'menciona',\n",
       " 'revisao',\n",
       " 'copia',\n",
       " 'confirmou',\n",
       " 'pensam',\n",
       " 'entretenimento',\n",
       " 'sumario',\n",
       " 'externa',\n",
       " 'classe',\n",
       " 'atencao',\n",
       " 'teorias',\n",
       " 'corpo',\n",
       " 'felizmente',\n",
       " 'leves',\n",
       " 'comecamos',\n",
       " 'confortavel',\n",
       " 'flashbacks',\n",
       " 'podia',\n",
       " 'leva',\n",
       " 'repetiam',\n",
       " 'tratado',\n",
       " 'alma',\n",
       " 'embalado',\n",
       " 'sentem',\n",
       " 'alguem',\n",
       " 'entregar',\n",
       " 'completo',\n",
       " 'diante',\n",
       " 'encontramos',\n",
       " 'beirando',\n",
       " 'comercializando',\n",
       " 'previsivel',\n",
       " 'ultimamente',\n",
       " 'contemporanea',\n",
       " '609',\n",
       " 'lia',\n",
       " 'envio',\n",
       " 'tccs',\n",
       " 'cabelo',\n",
       " 'facilita',\n",
       " 'eficiencia',\n",
       " 'nesse',\n",
       " 'hifenizadas',\n",
       " 'compreendido',\n",
       " 'anda',\n",
       " 'ponto',\n",
       " 'carros',\n",
       " 'confiaveis',\n",
       " 'reis',\n",
       " 'enrolado',\n",
       " 'fiz',\n",
       " 'desconstrucao',\n",
       " 'importante',\n",
       " 'verdade',\n",
       " 'necessariamente',\n",
       " 'best',\n",
       " 'surrada',\n",
       " 'garoto',\n",
       " 'ondulacoes',\n",
       " 'repugnante',\n",
       " 'tido',\n",
       " 'praxe',\n",
       " 'futeis',\n",
       " 'trono',\n",
       " 'lancamento',\n",
       " 'digitais',\n",
       " 'ceder',\n",
       " 'semanas',\n",
       " 'paragrafos',\n",
       " 'respondendo',\n",
       " 'referese',\n",
       " 'veiome',\n",
       " 'senti',\n",
       " 'melhor',\n",
       " 'degradante',\n",
       " 'extremidades',\n",
       " 'iniciantes',\n",
       " 'tragedias',\n",
       " 'sozinha',\n",
       " 'mark',\n",
       " 'geram',\n",
       " 'africanos',\n",
       " 'apimentada',\n",
       " 'vende',\n",
       " 'geral',\n",
       " 'votar',\n",
       " 'contradicao',\n",
       " 'democracia',\n",
       " 'fan',\n",
       " 'poderia',\n",
       " 'ficando',\n",
       " 'obivias',\n",
       " 'escolha',\n",
       " 'cristaos',\n",
       " 'disponibilizado',\n",
       " 'vitimizacao',\n",
       " 'citados',\n",
       " 'latim',\n",
       " 'orientais',\n",
       " 'possuo',\n",
       " 'irritante',\n",
       " 'distracao',\n",
       " 'novela',\n",
       " 'senhor',\n",
       " 'prosseguindo',\n",
       " 'acontecendo',\n",
       " 'publicado',\n",
       " 'arrependimento',\n",
       " 'subliminares',\n",
       " 'iriam',\n",
       " 'procure',\n",
       " 'todas',\n",
       " 'simultaneos',\n",
       " 'sinto',\n",
       " 'ontem',\n",
       " 'rasgado',\n",
       " 'traducao',\n",
       " 'coitadinha',\n",
       " 'selecao',\n",
       " 'apresenta',\n",
       " 'there',\n",
       " 'wil',\n",
       " 'natal',\n",
       " 'serve',\n",
       " 'perceberia',\n",
       " 'superou',\n",
       " 'filhinho',\n",
       " 'patria',\n",
       " 'leve',\n",
       " 'primaria',\n",
       " 'marquei',\n",
       " 'somente',\n",
       " 'tela',\n",
       " 'cinco',\n",
       " 'desinteressante',\n",
       " 'presosalva',\n",
       " 'tomou',\n",
       " 'concluidos',\n",
       " 'cary',\n",
       " 'pequenos',\n",
       " 'abandonar',\n",
       " 'olho',\n",
       " 'apela',\n",
       " 'pessoalmente',\n",
       " 'cientifica',\n",
       " 'fizeram',\n",
       " 'consiga',\n",
       " 'tia',\n",
       " 'modificados',\n",
       " 'brincadeira',\n",
       " 'atracao',\n",
       " 'desprezei',\n",
       " 'tipicas',\n",
       " 'salvasse',\n",
       " 'eficacia',\n",
       " 'aceita',\n",
       " 'procurando',\n",
       " 'possivelmente',\n",
       " 'grudadas',\n",
       " 'machucada',\n",
       " 'perdida',\n",
       " 'descontraida',\n",
       " 'consumidor',\n",
       " 'cronicas',\n",
       " 'lugarcomum',\n",
       " 'minimizamos',\n",
       " 'poesias',\n",
       " 'consegue',\n",
       " 'maravilhosa',\n",
       " 'motivacionais',\n",
       " 'acontecido',\n",
       " 'cheguem',\n",
       " 'zelo',\n",
       " 'dolorida',\n",
       " 'logicas',\n",
       " 'acao',\n",
       " 'embrulho',\n",
       " 'perseguidor',\n",
       " 'pastor',\n",
       " 'entender',\n",
       " 'erotico',\n",
       " 'repeti',\n",
       " 'pesssimo',\n",
       " 'tirando',\n",
       " 'futuro',\n",
       " 'escroto',\n",
       " 'not',\n",
       " 'interacao',\n",
       " 'encontrado',\n",
       " 'recomendado',\n",
       " 'felipe',\n",
       " 'publicacoes',\n",
       " 'atrativo',\n",
       " 'escorrega',\n",
       " 'lido',\n",
       " 'reembolso',\n",
       " 'aneis',\n",
       " '90',\n",
       " 'destilacao',\n",
       " 'tomei',\n",
       " 'vidro',\n",
       " 'anthony',\n",
       " 'desservico',\n",
       " 'abismado',\n",
       " 'sofrivel',\n",
       " 'desrtruir',\n",
       " 'rotuladas',\n",
       " 'cm',\n",
       " 'desejar',\n",
       " 'oferecido',\n",
       " 'acerca',\n",
       " 'cuidado',\n",
       " 'concerto',\n",
       " 'especie',\n",
       " 'bolso',\n",
       " 'soltasa',\n",
       " 'soar',\n",
       " 'adquirila',\n",
       " 'reparo',\n",
       " 'branca',\n",
       " 'manipular',\n",
       " 'fausto',\n",
       " 'precisa',\n",
       " 'surgir',\n",
       " 'espuma',\n",
       " 'inventar',\n",
       " 'vendela',\n",
       " 'filha',\n",
       " 'marca',\n",
       " 'poucos',\n",
       " 'malintencionadas',\n",
       " 'anotei',\n",
       " 'descricao',\n",
       " 'graca',\n",
       " 'barato',\n",
       " 'devem',\n",
       " 'precisou',\n",
       " 'avaliar',\n",
       " 'forcado',\n",
       " 'violencia',\n",
       " 'digitalizar',\n",
       " 'associacoes',\n",
       " \"'falam\",\n",
       " 'amassadas',\n",
       " 'manutencao',\n",
       " 'usualmente',\n",
       " 'queiroz',\n",
       " 'consideravelmente',\n",
       " 'feita',\n",
       " 'chamado',\n",
       " 'amassos',\n",
       " 'oficial',\n",
       " 'contagiado',\n",
       " 'lanca',\n",
       " 'verosimilhanca',\n",
       " 'sair',\n",
       " 'rebuscadas',\n",
       " 'fenix',\n",
       " 'temo',\n",
       " 'apresentado',\n",
       " 'picantes',\n",
       " 'cristiano',\n",
       " 'reivindicando',\n",
       " 'exesposa',\n",
       " 'relacoes',\n",
       " 'pestanejar',\n",
       " 'apostilas',\n",
       " 'realidade',\n",
       " 'paginas',\n",
       " 'la',\n",
       " 'anotado',\n",
       " 'r20',\n",
       " 'serem',\n",
       " 'vivas',\n",
       " 'acabam',\n",
       " 'meios',\n",
       " 'abadia',\n",
       " 'rota',\n",
       " 'plastico',\n",
       " 'ultima',\n",
       " 'retratando',\n",
       " 'comedia',\n",
       " 'empoeirados',\n",
       " 'emitiram',\n",
       " 'levantar',\n",
       " 'motivos',\n",
       " 'amado',\n",
       " 'continua',\n",
       " 'amontoado',\n",
       " 'perto',\n",
       " 'faculdade',\n",
       " 'explicacoes',\n",
       " 'acessiveis',\n",
       " 'fila',\n",
       " 'feio',\n",
       " 'distribuido',\n",
       " 'aibda',\n",
       " 'andam',\n",
       " 'encontrou',\n",
       " 'femininas',\n",
       " 'aprovouo',\n",
       " 'caminhoneiros',\n",
       " 'americana',\n",
       " 'fez',\n",
       " 'usadas',\n",
       " 'esquecem',\n",
       " 'coagir',\n",
       " 'mentirosas',\n",
       " 'traduzida',\n",
       " 'chamada',\n",
       " 'fisica',\n",
       " 'respondi',\n",
       " 'adorei',\n",
       " 'va',\n",
       " 'sente',\n",
       " 'nenhum',\n",
       " 'justificativa',\n",
       " 'livre',\n",
       " 'subestima',\n",
       " 'peguei',\n",
       " 'acho',\n",
       " '2outros',\n",
       " 'memoria',\n",
       " 'comparavel',\n",
       " 'percebi',\n",
       " 'caro',\n",
       " 'eterna',\n",
       " 'xingam',\n",
       " 'anima',\n",
       " 'pontuacao',\n",
       " 'comporta',\n",
       " 'ee',\n",
       " 'download',\n",
       " 'objetivo',\n",
       " 'algum',\n",
       " 'pottermore',\n",
       " 'narracao',\n",
       " 'percebo',\n",
       " 'bastante',\n",
       " 'diagramacao',\n",
       " 'cronologicamente',\n",
       " 'robbins',\n",
       " 'seriamente',\n",
       " 'janeiro',\n",
       " 'desrespeitando',\n",
       " 'custar',\n",
       " 'desejo',\n",
       " 'escolhi',\n",
       " 'aplicacao',\n",
       " 'desonesta',\n",
       " 'manipulador',\n",
       " 'ocorrer',\n",
       " 'concepcao',\n",
       " 'atendida',\n",
       " 'polonia',\n",
       " 'arrasta',\n",
       " 'recemsaido',\n",
       " 'alheios',\n",
       " 'obviamente',\n",
       " 'protagonistas',\n",
       " 'belas',\n",
       " 'costumo',\n",
       " 'seguidores',\n",
       " 'comparandose',\n",
       " 'familia',\n",
       " 'apresentava',\n",
       " '3',\n",
       " 'laterais',\n",
       " 'ne',\n",
       " 'unidos',\n",
       " 'insights',\n",
       " 'trabalhar',\n",
       " 'enfrentando',\n",
       " 'charlatao',\n",
       " 'perdurou',\n",
       " 'similar',\n",
       " 'fracassar',\n",
       " 'decepcionada',\n",
       " 'casado',\n",
       " 'virgem',\n",
       " 'rasgos',\n",
       " 'godrico',\n",
       " 'reduzido',\n",
       " 'fotos',\n",
       " 'contracapa',\n",
       " 'positivas',\n",
       " 'cuida',\n",
       " 'sozinho',\n",
       " 'vomitar',\n",
       " 'determinado',\n",
       " 'entreguei',\n",
       " 'reflexoes',\n",
       " 'metodo',\n",
       " 'desrespeitosa',\n",
       " 'ingenuos',\n",
       " 'consultor',\n",
       " 'bolsa',\n",
       " 'preguicoso',\n",
       " 'mencionadas',\n",
       " 'absurdo',\n",
       " 'sociedade',\n",
       " 'gasta',\n",
       " 'animo',\n",
       " 'apanhava',\n",
       " 'cortadas',\n",
       " 'contradicoes',\n",
       " 'livro',\n",
       " 'corrobora',\n",
       " 'internet',\n",
       " 'aventura',\n",
       " 'fonte',\n",
       " 'recuperar',\n",
       " 'viver',\n",
       " 'ambos',\n",
       " 'super',\n",
       " 'comprometidos',\n",
       " 'it',\n",
       " 'prega',\n",
       " 'valeriam',\n",
       " 'assiduo',\n",
       " \"sant'anna\",\n",
       " 'encontraram',\n",
       " 'jorge',\n",
       " 'recomendacoes',\n",
       " 'devido',\n",
       " 'centenas',\n",
       " 'impecavel',\n",
       " 'roteiro',\n",
       " 'penhasco',\n",
       " 'pratico',\n",
       " 'autoestima',\n",
       " 'trocou',\n",
       " '22',\n",
       " 'concisa',\n",
       " 'ocorrem',\n",
       " 'acertar',\n",
       " 'continuando',\n",
       " 'presta',\n",
       " 'enorme',\n",
       " 'trasse',\n",
       " 'pesquisar',\n",
       " 'subjugar',\n",
       " 'fixar',\n",
       " 'bordas',\n",
       " 'originais',\n",
       " 'assemelha',\n",
       " 'eai',\n",
       " 'passaram',\n",
       " 'materiais',\n",
       " 'intangivel',\n",
       " 'dobrada',\n",
       " 'pesada',\n",
       " 'ocupam',\n",
       " 'austen',\n",
       " 'poema',\n",
       " 'brasileiras',\n",
       " 'chegaram',\n",
       " 'existia',\n",
       " 'fundo',\n",
       " 'separadamente',\n",
       " 'dizer',\n",
       " 'necessarias',\n",
       " 'diario',\n",
       " 'eco',\n",
       " 'ameacatipo',\n",
       " 'lerja',\n",
       " 'eou',\n",
       " 'queda',\n",
       " 'ressaltar',\n",
       " 'indios',\n",
       " 'pocao',\n",
       " 'expectativa',\n",
       " 'prendem',\n",
       " 'estudante',\n",
       " 'reproduzir',\n",
       " 'chance',\n",
       " 'publicaram',\n",
       " 'dedo',\n",
       " 'hugo',\n",
       " 'atrair',\n",
       " 'mesa',\n",
       " 'carregar',\n",
       " 'formar',\n",
       " 'base',\n",
       " 'ajustem',\n",
       " 'compara',\n",
       " 'ouvindo',\n",
       " 'excelentee',\n",
       " 'contando',\n",
       " 'espontanea',\n",
       " 'definitivamente',\n",
       " 'tratou',\n",
       " 'danificou',\n",
       " 'aguardada',\n",
       " 'mudem',\n",
       " 'cheguei',\n",
       " 'mesquinha',\n",
       " 'solicitava',\n",
       " 'enfim',\n",
       " 'piadinhas',\n",
       " 'bem',\n",
       " 'post',\n",
       " 'endereco',\n",
       " 'trey',\n",
       " 'claret',\n",
       " 'descontextualizadas',\n",
       " 'atendimento',\n",
       " 'ora',\n",
       " 'livrar',\n",
       " 'i',\n",
       " 'escreve',\n",
       " 'familiar',\n",
       " 'lair',\n",
       " 'pode',\n",
       " 'acabava',\n",
       " 'cuidadosa',\n",
       " 'jornalistas',\n",
       " 'cartao',\n",
       " 'principios',\n",
       " 'cotidiano',\n",
       " 'jaa',\n",
       " 'militante',\n",
       " 'classico',\n",
       " 'estipulados',\n",
       " 'ditatoriais',\n",
       " 'kindle',\n",
       " 'rascunho',\n",
       " 'bela',\n",
       " 'infudados',\n",
       " 'gostam',\n",
       " 'julgue',\n",
       " 'inclusao',\n",
       " 'descricoes',\n",
       " 'aula',\n",
       " 'processo',\n",
       " 'vender',\n",
       " 'ah',\n",
       " 'capitaliza',\n",
       " 'gibis',\n",
       " '4158',\n",
       " 'perdemos',\n",
       " 'amacadas',\n",
       " 'contrariem',\n",
       " 'comunistinha',\n",
       " 'place',\n",
       " 'corri',\n",
       " 'fa',\n",
       " 'online',\n",
       " 'desapontou',\n",
       " 'gama',\n",
       " 'incomodo',\n",
       " 'pessimo',\n",
       " 'comidas',\n",
       " 'comprovado',\n",
       " 'dominantes',\n",
       " 'textual',\n",
       " 'correlacionando',\n",
       " 'gelo',\n",
       " 'hominideo',\n",
       " 'protecao',\n",
       " 'adultos',\n",
       " 'diversidade',\n",
       " 'folha',\n",
       " 'libertarian',\n",
       " 'despertou',\n",
       " 'conturbado',\n",
       " 'novembro',\n",
       " 'qualquer',\n",
       " 'interessantes',\n",
       " 'ilustrativa',\n",
       " 'lino',\n",
       " 'fraudulento',\n",
       " 'tais',\n",
       " 'motivada',\n",
       " 'quanto',\n",
       " 'intrigante',\n",
       " 'justamente',\n",
       " 'primeiros',\n",
       " 'sabem',\n",
       " 'criar',\n",
       " 'alcoolatra',\n",
       " 'banal',\n",
       " 'estrutura',\n",
       " 'respeito',\n",
       " 'preferiu',\n",
       " 'historietas',\n",
       " 'dessa',\n",
       " '28122017',\n",
       " 'desorganizada',\n",
       " 'entregues',\n",
       " 'existee',\n",
       " 'explicacao',\n",
       " 'incriveis',\n",
       " 'perceber',\n",
       " 'julgar',\n",
       " 'estreitas',\n",
       " 'impressiona',\n",
       " 'considerado',\n",
       " 'real',\n",
       " 'citado',\n",
       " 'comecei',\n",
       " 'velocidade',\n",
       " 'primordios',\n",
       " 'convence',\n",
       " 'entediante',\n",
       " 'comentar',\n",
       " 'lendo',\n",
       " 'tolerancia',\n",
       " 'repentinamente',\n",
       " 'laranja',\n",
       " 'cumprindo',\n",
       " 'usar',\n",
       " 'julga',\n",
       " 'chocada',\n",
       " 'anteriores',\n",
       " 'naada',\n",
       " 'completamente',\n",
       " 'sugeria',\n",
       " 'ligacao',\n",
       " 'superior',\n",
       " 'aplicam',\n",
       " 'leia',\n",
       " 'dou',\n",
       " 'fator',\n",
       " 'bebem',\n",
       " 'folhas',\n",
       " 'trocas',\n",
       " 'disponivel',\n",
       " 'q',\n",
       " 'lamentavel',\n",
       " 'miracle',\n",
       " 'mesclas',\n",
       " 'formatacoes',\n",
       " 'cultural',\n",
       " 'empolgou',\n",
       " 'almeida',\n",
       " 'reconhecer',\n",
       " 'exemplar',\n",
       " 'reflexivos',\n",
       " 'brilhante',\n",
       " 'cativa',\n",
       " 'dezenas',\n",
       " 'background',\n",
       " 'reembolsoe',\n",
       " 'convidados',\n",
       " '2x',\n",
       " 'igual',\n",
       " 'ameacas',\n",
       " 'falam',\n",
       " 'gratuito',\n",
       " 'toque',\n",
       " 'elogios',\n",
       " 'deseja',\n",
       " 'fluem',\n",
       " 'direcao',\n",
       " 'sam',\n",
       " 'relatada',\n",
       " 'astral',\n",
       " 'chatoprotagonistas',\n",
       " 'corrobar',\n",
       " 'li',\n",
       " 'corda',\n",
       " 'familiares',\n",
       " 'compreendo',\n",
       " 'altas',\n",
       " 'cansamos',\n",
       " 'prestar',\n",
       " 'futilidade',\n",
       " 'entrada',\n",
       " 'exagero',\n",
       " 'extraido',\n",
       " 'tris',\n",
       " 'descartavel',\n",
       " 'tolos',\n",
       " 'apresentando',\n",
       " 'mimada',\n",
       " 'visao',\n",
       " 'premio',\n",
       " 'trocadinhos',\n",
       " 'formato',\n",
       " 'encomenda',\n",
       " 'ideal',\n",
       " 'prosa',\n",
       " 'pensando',\n",
       " 'envolve',\n",
       " 'intacto',\n",
       " 'excelente',\n",
       " 'sentiria',\n",
       " 'tonta',\n",
       " 'posicionados',\n",
       " 'famoso',\n",
       " 'restringira',\n",
       " 'ridicula',\n",
       " '14',\n",
       " 'tentativa',\n",
       " 'nomes',\n",
       " 'assustadora',\n",
       " 'pior',\n",
       " 'contado',\n",
       " 'cortella',\n",
       " 'cura',\n",
       " 'bacanaque',\n",
       " 'pensamento',\n",
       " 'acreditava',\n",
       " 'soltaram',\n",
       " 'diagramado',\n",
       " 'promessa',\n",
       " 'abusos',\n",
       " 'outros',\n",
       " 'escorar',\n",
       " 'facame',\n",
       " 'valeu',\n",
       " 'proposto',\n",
       " 'compromisso',\n",
       " 'gigante',\n",
       " 'situacoes',\n",
       " 'comprarem',\n",
       " 'indiretas',\n",
       " 'empolgante',\n",
       " 'significado',\n",
       " 'editados',\n",
       " 'relembrando',\n",
       " 'fobia',\n",
       " 'veio',\n",
       " 'apena',\n",
       " 'papo',\n",
       " 'diferentes',\n",
       " 'cito',\n",
       " 'convencendo',\n",
       " 'banalizacao',\n",
       " 'uktima',\n",
       " 'will',\n",
       " 'clarke',\n",
       " 'new',\n",
       " 'males',\n",
       " 'ficcao',\n",
       " 'saindo',\n",
       " 'tocala',\n",
       " 'estabelecer',\n",
       " 'originalidadetanto',\n",
       " 'aceitem',\n",
       " 'guardaria',\n",
       " 'pessoa',\n",
       " 'viagem',\n",
       " 'tomando',\n",
       " 'cartas',\n",
       " 'ficarem',\n",
       " 'espero',\n",
       " 'alemanha',\n",
       " 'email',\n",
       " 'sitar',\n",
       " 'casadas',\n",
       " 'encontro',\n",
       " 'referente',\n",
       " 'inicial',\n",
       " 'ir',\n",
       " 'livreiro',\n",
       " 'sofia',\n",
       " 'mensagem',\n",
       " 'fic',\n",
       " 'demais',\n",
       " 'guerras',\n",
       " 'conversa',\n",
       " 'cristianismo',\n",
       " 'implica',\n",
       " '2019',\n",
       " 'traduz',\n",
       " 'media',\n",
       " 'anedotas',\n",
       " 'deram',\n",
       " 'aguardando',\n",
       " 'visionario',\n",
       " 'contribuir',\n",
       " 'embora',\n",
       " 'horas',\n",
       " 'decisivo',\n",
       " 'intitula',\n",
       " 'ai',\n",
       " 'insiste',\n",
       " 'promove',\n",
       " 'deleitar',\n",
       " 'louca',\n",
       " 'sofredora',\n",
       " 'reconhecidamente',\n",
       " 'apostei',\n",
       " 'estoria',\n",
       " 'desrespeito',\n",
       " 'autopromocao',\n",
       " 'devia',\n",
       " 'brutalidade',\n",
       " 'recomendoi',\n",
       " 'termos',\n",
       " 'incorporar',\n",
       " 'manchas',\n",
       " 'compensou',\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Vou criar o dicionário de todas as palavras e contabilizar\n",
    "\n",
    "vocabulario = cria_vocabulario(train.Mensagem)  #retornos uma lista com todas as palavras\n",
    "vocabulario = set(vocabulario)  #retira as palavras repetidas\n",
    "vocabulario\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance do Classificador\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu querer querer\n"
     ]
    }
   ],
   "source": [
    "def lemmat(texto):\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    lemmat_radicais = []\n",
    "\n",
    "    for radicais in doc:\n",
    "        lemmat_radicais.append(radicais.lemma_)\n",
    "    \n",
    "    texto_lemmat = ' '.join(lemmat_radicais)\n",
    "    \n",
    "    return texto_lemmat\n",
    "\n",
    "print(lemmat(\"eu quero querer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a probabilidade de todas as palavras estarem contidas num texto não é a mesma? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Qualidade do Classificador a partir de novas separações dos tweets entre Treinamento e Teste\n",
    "\n",
    "Caso for fazer esse item do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aperfeiçoamento:\n",
    "\n",
    "Trabalhos que conseguirem pelo menos conceito B vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* IMPLEMENTOU outras limpezas e transformações que não afetem a qualidade da informação contida nos tweets. Ex: stemming, lemmatization, stopwords\n",
    "* CONSIDEROU mais de duas categorias na variável Target e INCREMENTOU a quantidade de notícias, mantendo pelo menos 250 notícias por categoria (OBRIGATÓRIO PARA TRIOS, sem contar como item avançado)\n",
    "* Para Target com duas categorias: CRIOU pelo menos quatro categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante\n",
    "* EXPLICOU porquê não pode usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* PROPÔS diferentes cenários para Naïve Bayes fora do contexto do projeto (pelo menos dois cenários, exceto aqueles já apresentados em sala pelos professores: por exemplo, filtro de spam)\n",
    "* SUGERIU e EXPLICOU melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* FEZ o item Qualidade do Classificador a partir de novas separações das Notícias entre Treinamento e Teste descrito no enunciado do projeto (OBRIGATÓRIO para conceitos A ou A+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
